{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features Quiz\n",
    "Use this Jupyter notebook to find the answers to the quiz in the previous section. There is an answer key in the next part of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \\\n",
    "    IDF, StringIndexer\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler\n",
    "from pyspark.sql.functions import udf, sum as Fsum, pow as Fpow, col, sqrt as Fsqrt\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Creating Features\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_overflow_data = 'Train_onetag_small.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Body: string, Id: bigint, Tags: string, Title: string, oneTag: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(stack_overflow_data)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- oneTag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Body Length Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"Body\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "df = regexTokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_length = udf(lambda x: len(x), IntegerType())\n",
    "df = df.withColumn(\"BodyLength\", body_length(df.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\", Id=1, Tags='php image-processing file-upload upload mime-types', Title='How to check if an uploaded file is an image without mime type?', oneTag='php', words=['p', 'i', 'd', 'like', 'to', 'check', 'if', 'an', 'uploaded', 'file', 'is', 'an', 'image', 'file', 'e', 'g', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'or', 'another', 'file', 'the', 'problem', 'is', 'that', 'i', 'm', 'using', 'uploadify', 'to', 'upload', 'the', 'files', 'which', 'changes', 'the', 'mime', 'type', 'and', 'gives', 'a', 'text', 'octal', 'or', 'something', 'as', 'the', 'mime', 'type', 'no', 'matter', 'which', 'file', 'type', 'you', 'upload', 'p', 'p', 'is', 'there', 'a', 'way', 'to', 'check', 'if', 'the', 'uploaded', 'file', 'is', 'an', 'image', 'apart', 'from', 'checking', 'the', 'file', 'extension', 'using', 'php', 'p'], BodyLength=83)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+----------------+--------------------+----------+\n",
      "|                Body| Id|                Tags|               Title|          oneTag|               words|BodyLength|\n",
      "+--------------------+---+--------------------+--------------------+----------------+--------------------+----------+\n",
      "|<p>I'd like to ch...|  1|php image-process...|How to check if a...|             php|[p, i, d, like, t...|        83|\n",
      "|<p>In my favorite...|  2|             firefox|How can I prevent...|         firefox|[p, in, my, favor...|        71|\n",
      "|<p>I am import ma...|  3|r matlab machine-...|R Error Invalid t...|               r|[p, i, am, import...|      3161|\n",
      "|<p>This is probab...|  4|     c# url encoding|How do I replace ...|              c#|[p, this, is, pro...|       115|\n",
      "|<pre><code>functi...|  5|php api file-get-...|How to modify who...|             php|[pre, code, funct...|       148|\n",
      "|<p>I am using a m...|  6|proxy active-dire...|setting proxy in ...|active-directory|[p, i, am, using,...|        69|\n",
      "|<p>My image is ca...|  7|           core-plot|How to draw barpl...|           other|[p, my, image, is...|       112|\n",
      "|<p>I've decided t...|  8|c# asp.net window...|How to fetch an X...|              c#|[p, i, ve, decide...|       161|\n",
      "|<p>Do you know of...|  9|.net javascript c...|.NET library for ...|      javascript|[p, do, you, know...|       102|\n",
      "|<p>I'm using SQL ...| 10|sql variables par...|SQL Server : proc...|             sql|[p, i, m, using, ...|        67|\n",
      "|<p>Some commercia...| 11|.net obfuscation ...|How do commercial...|            .net|[p, some, commerc...|       134|\n",
      "|<p>This may sound...| 12|algorithm languag...|Crappy Random Num...|       algorithm|[p, this, may, so...|       144|\n",
      "|<p>how can I move...| 13|postfix migration...|Migrate from Mdae...|           other|[p, how, can, i, ...|        19|\n",
      "|<p>Few month ago ...| 14|documentation lat...|Where can I find ...|   documentation|[p, few, month, a...|        42|\n",
      "|<p>When you hit a...| 15|           windows-7|Can I stop window...|       windows-7|[p, when, you, hi...|        37|\n",
      "|<p>A lot of frame...| 16|php url-routing c...|PHP framework URL...|             php|[p, a, lot, of, f...|       385|\n",
      "|<p>I'm running a ...| 17|   r temporary-files|What creates \".rd...|               r|[p, i, m, running...|       373|\n",
      "|<p>Hello<br>\n",
      "I'm ...| 18|         wpf binding|WPF: multiple con...|             wpf|[p, hello, br, i,...|       209|\n",
      "|<p>Does anyone kn...| 19|javascript code-g...|Play framework au...|      javascript|[p, does, anyone,...|       116|\n",
      "|<p>=) I need your...| 20|php xml hash mult...|Creating a repeti...|             php|[p, i, need, your...|       390|\n",
      "+--------------------+---+--------------------+--------------------+----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Select the question with Id = 1112. How many words does its body contain (check the BodyLength column)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|  Id|BodyLength|\n",
      "+----+----------+\n",
      "|1112|        63|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"Id\", \"BodyLength\"]).filter(df.Id==1112).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Create a new column that concatenates the question title and body. Apply the same functions we used before to compute the number of words in this combined column. What's the value in this new column for Id = 5123?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = udf(lambda x, y: x + ' ' + y)\n",
    "df = df.withColumn(\"body_title\", concat(df.Body, df.Title))\n",
    "# another way\n",
    "# df = df.withColumn(\"Desc\", concat(col(\"Title\"), lit(' '), col(\"Body\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(body_title=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n How to check if an uploaded file is an image without mime type?\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"body_title\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Body\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Title='How to check if an uploaded file is an image without mime type?')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Title\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"body_title_length\", body_length(df.body_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+\n",
      "|  Id|BodyLength|body_title_length|\n",
      "+----+----------+-----------------+\n",
      "|5123|       132|              135|\n",
      "+----+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer2 = RegexTokenizer(inputCol=\"body_title\", outputCol=\"words2\", pattern=\"\\\\W\")\n",
    "df = regexTokenizer2.transform(df)\n",
    "df = df.withColumn(\"body_title_length\", body_length(df.words2))\n",
    "df.select([\"Id\", \"BodyLength\", \"body_title_length\"]).filter(df.Id==5123).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Vector\n",
    "Create a vector from the combined Title + Body length column. In the next few questions, you'll try different normalizer/scaler methods on this new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"body_title_length\"], outputCol=\"NumFeatures\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+-------+--------------------+----------+--------------------+-----------------+--------------------+-----------+\n",
      "|                Body| Id|                Tags|               Title| oneTag|               words|BodyLength|          body_title|body_title_length|              words2|NumFeatures|\n",
      "+--------------------+---+--------------------+--------------------+-------+--------------------+----------+--------------------+-----------------+--------------------+-----------+\n",
      "|<p>I'd like to ch...|  1|php image-process...|How to check if a...|    php|[p, i, d, like, t...|        83|<p>I'd like to ch...|               96|[p, i, d, like, t...|     [96.0]|\n",
      "|<p>In my favorite...|  2|             firefox|How can I prevent...|firefox|[p, in, my, favor...|        71|<p>In my favorite...|               83|[p, in, my, favor...|     [83.0]|\n",
      "+--------------------+---+--------------------+--------------------+-------+--------------------+----------+--------------------+-----------------+--------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Using the Normalizer method what's the normalized value for question Id = 512?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\")\n",
    "df = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------------+\n",
      "| Id|NumFeatures|ScaledNumFeatures|\n",
      "+---+-----------+-----------------+\n",
      "|512|     [57.0]|            [1.0]|\n",
      "+---+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"Id\", \"NumFeatures\", \"ScaledNumFeatures\"]).filter(df.Id==512).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|ScaledNumFeatures|\n",
      "+-----------------+\n",
      "|            [1.0]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"ScaledNumFeatures\"]).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------------+\n",
      "|body_title_length|POWER(body_title_length, 2.0)|\n",
      "+-----------------+-----------------------------+\n",
      "|               96|                       9216.0|\n",
      "|               83|                       6889.0|\n",
      "|             3168|                  1.0036224E7|\n",
      "|              124|                      15376.0|\n",
      "|              154|                      23716.0|\n",
      "|               75|                       5625.0|\n",
      "|              121|                      14641.0|\n",
      "|              170|                      28900.0|\n",
      "|              107|                      11449.0|\n",
      "|               74|                       5476.0|\n",
      "|              145|                      21025.0|\n",
      "|              148|                      21904.0|\n",
      "|               24|                        576.0|\n",
      "|               49|                       2401.0|\n",
      "|               48|                       2304.0|\n",
      "|              389|                     151321.0|\n",
      "|              380|                     144400.0|\n",
      "|              216|                      46656.0|\n",
      "|              123|                      15129.0|\n",
      "|              404|                     163216.0|\n",
      "+-----------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fpow(df.select(\"NumFeatures\"),2)\n",
    "df.select(\"body_title_length\", Fpow(col(\"body_title_length\"), 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83310.708981498887"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def sum_col(df, col_):\n",
    "    return np.sqrt((df.select(F.sum(F.pow(col(col_), 2))).collect()[0][0]))\n",
    "sum_col(df, 'body_title_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = Q1.groupBy().agg(F.sum(\"cpih_coicop_weight\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqaured = udf(lambda x: x*x, IntegerType()) #df.body_title_length/Fsqrt(Fsum(sqaured(df.body_title_length)))\n",
    "df = df.withColumn(\"normalized_\", col(\"body_title_length\")/sum_col(df, 'body_title_length'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|body_title_length|         normalized_|\n",
      "+-----------------+--------------------+\n",
      "|               96|0.001152312843974...|\n",
      "|               83|9.962704796862563E-4|\n",
      "|             3168|0.038026323851157354|\n",
      "|              124|0.001488404090133...|\n",
      "|              154|0.001848501853875...|\n",
      "|               75|9.002444093550509E-4|\n",
      "|              121|0.001452394313759...|\n",
      "|              170|0.002040553994538...|\n",
      "|              107|0.001284348690679...|\n",
      "|               74|8.882411505636502E-4|\n",
      "|              145|0.001740472524753...|\n",
      "|              148|0.001776482301127...|\n",
      "|               24|2.880782109936163E-4|\n",
      "|               49|5.881596807786332E-4|\n",
      "|               48|5.761564219872326E-4|\n",
      "|              389|0.004669267669854864|\n",
      "|              380|0.004561238340732258|\n",
      "|              216|0.002592703898942...|\n",
      "|              123|0.001476400831342...|\n",
      "|              404|0.004849316551725874|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"body_title_length\",\"normalized_\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|  sum(normalized_)|\n",
      "+------------------+\n",
      "|216.39699410076344|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.sum(\"normalized_\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(normalized_)=216.39699410076344)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy().agg(F.sum(\"normalized_\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(POWER(body_title_length, 2.0))=6940674231.0)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy().agg(F.sum(F.pow(col(\"body_title_length\"), 2))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SQRT(sum(POWER(body_title_length, 2.0)))=83310.70898149889)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy().agg(F.sqrt(F.sum(F.pow(col(\"body_title_length\"), 2)))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not exactly sure wht does not do this correctly (as expected), is it because in  [] form? above way is to do it manually!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures_p1\", p=1.0)\n",
    "df = normalizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+\n",
      "| Id|NumFeatures|ScaledNumFeatures_p1|\n",
      "+---+-----------+--------------------+\n",
      "|512|     [57.0]|               [1.0]|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"Id\", \"NumFeatures\", \"ScaledNumFeatures_p1\"]).filter(df.Id==512).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Using the StandardScaler method (**scaling both the mean and the standard deviation**) what's the normalized value for question Id = 512?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"ScaledNumFeatures2\")\n",
    "scaler2 = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures2\", withStd=True, withMean=True)\n",
    "scalerModel = scaler2.fit(df)\n",
    "df = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+\n",
      "| Id|NumFeatures|  ScaledNumFeatures2|\n",
      "+---+-----------+--------------------+\n",
      "|512|     [57.0]|[-0.6417314460998...|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"Id\", \"NumFeatures\", \"ScaledNumFeatures2\"]).filter(df.Id==512).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Using the MinMAxScaler method what's the normalized value for question Id = 512?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures3\")\n",
    "scalerModel = scaler.fit(df)\n",
    "df = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+\n",
      "| Id|NumFeatures|  ScaledNumFeatures3|\n",
      "+---+-----------+--------------------+\n",
      "|512|     [57.0]|[0.00624833820792...|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"Id\", \"NumFeatures\", \"ScaledNumFeatures3\"]).filter(df.Id==512).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary| body_title_length|\n",
      "+-------+------------------+\n",
      "|  count|            100000|\n",
      "|   mean|         180.28187|\n",
      "| stddev|192.10819533505023|\n",
      "|    min|                10|\n",
      "|    max|              7532|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"body_title_length\").describe().show() # describe can't be done on vector form, need the raw number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct : (57-10)/(7532-10)=0.0062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id=0, features=DenseVector([1.0, 0.5, -1.0]), normFeatures2=DenseVector([0.6667, 0.3333, -0.6667]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures2\", p=2.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id=0, features=DenseVector([1.0]), normFeatures3=DenseVector([1.0]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0]),),\n",
    "    (1, Vectors.dense([2.0]),),\n",
    "    (2, Vectors.dense([4.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures3\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using min_max norm\n",
      "+---+--------+--------------------+\n",
      "| id|features|       normFeatures4|\n",
      "+---+--------+--------------------+\n",
      "|  0|   [1.0]|               [0.0]|\n",
      "|  1|   [2.0]|[0.3333333333333333]|\n",
      "|  2|   [4.0]|               [1.0]|\n",
      "+---+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0]),),\n",
    "    (1, Vectors.dense([2.0]),),\n",
    "    (2, Vectors.dense([4.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"normFeatures4\")\n",
    "scalerModel = normalizer.fit(dataFrame)\n",
    "l1NormData = scalerModel.transform(dataFrame)\n",
    "print(\"Normalized using min_max norm\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
